{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc52ea3c-f564-48fb-9c21-86d09f5d5190",
   "metadata": {},
   "source": [
    "# Step 1: import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237e5b7e-a589-4362-86bc-2b7144ff7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "import requests\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f23b4-cfc4-497a-9eb6-99085929f3b9",
   "metadata": {},
   "source": [
    "# Step 2: Prepare the corpus\n",
    "### Load, tokenize, and preprocess the Reuters dataset.\n",
    "### Build the vocab, inverse mapping and numericalise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03918a95-2205-4e8a-b057-2abb87a9f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ff63af-5794-448a-a1b6-f8b4f4f1222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xiaoxuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt') \n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
    "\n",
    "# Prepare the dataset\n",
    "response = requests.get(url)\n",
    "raw_text = response.text\n",
    "\n",
    "# Define markers\n",
    "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\"\n",
    "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\"\n",
    "\n",
    "# Find the start and end positions\n",
    "start_idx = raw_text.find(start_marker) + len(start_marker)\n",
    "end_idx = raw_text.find(end_marker)\n",
    "\n",
    "# Slice out the actual content\n",
    "text = raw_text[start_idx:end_idx].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdb9623e-b6c8-46b6-acdb-3735d9d18488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text length: 5555356 characters\n",
      "Preview:\n",
      " The Complete Works of William Shakespeare\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Contents\n",
      "\n",
      "    THE SONNETS\n",
      "    ALL’S WELL THAT ENDS WELL\n",
      "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      "    AS YOU LIKE IT\n",
      "    THE COMEDY OF ERRORS\n",
      "    THE TRAGEDY OF CORIOLANUS\n",
      "    CYMBELINE\n",
      "    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
      "    THE FIRST PART OF KING HENRY THE FOURTH\n",
      "    THE SECOND PART OF KING HENRY THE FOURTH\n",
      "    THE LIFE OF KING HENRY THE FIFTH\n",
      "    THE FIRST PART OF HENRY THE SIXTH\n",
      "    THE SECOND PART OF KING HENRY THE SIXTH\n",
      "    THE THIRD PART OF KING HENRY THE SIXTH\n",
      "    KING HENRY THE EIGHTH\n",
      "    THE LIFE AND DEATH OF KING JOHN\n",
      "    THE TRAGEDY OF JULIUS CAESAR\n",
      "    THE TRAGEDY OF KING LEAR\n",
      "    LOVE’S LABOUR’S LOST\n",
      "    THE TRAGEDY OF MACBETH\n",
      "    MEASURE FOR MEASURE\n",
      "    THE MERCHANT OF VENICE\n",
      "    THE MERRY WIVES OF WINDSOR\n",
      "    A MIDSUMMER NIGHT’S DREAM\n",
      "    MUCH ADO ABOUT NOTHING\n",
      "    THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\n",
      "    PERICLES, PRINCE OF TYRE\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "#---For checking---\n",
    "print(f\"Extracted text length: {len(text)} characters\")\n",
    "print(\"Preview:\\n\", text[:1000])  # Show first 1000 characters\n",
    "### End of checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52ec2f4f-ed2a-43b3-aacc-dce6a09da639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 104698\n",
      "Total words: 911436\n",
      "\n",
      "First 5 tokenized sentences:\n",
      "1: the complete works of william shakespeare by william shakespeare contents the sonnets all well that ends well the tragedy of antony and cleopatra as you like it the comedy of errors the tragedy of coriolanus cymbeline the tragedy of hamlet prince of denmark the first part of king henry the fourth the second part of king henry the fourth the life of king henry the fifth the first part of henry the sixth the second part of king henry the sixth the third part of king henry the sixth king henry the eighth the life and death of king john the tragedy of julius caesar the tragedy of king lear love labour lost the tragedy of macbeth measure for measure the merchant of venice the merry wives of windsor midsummer night dream much ado about nothing the tragedy of othello the moor of venice pericles prince of tyre king richard the second king richard the third the tragedy of romeo and juliet the taming of the shrew the tempest the life of timon of athens the tragedy of titus andronicus troilus and cressida twelfth night or what you will the two gentlemen of verona the two noble kinsmen the winter tale lover complaint the passionate pilgrim the phoenix and the turtle the rape of lucrece venus and adonis the sonnets from fairest creatures we desire increase that thereby beauty rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feed st thy light flame with fuel making famine where abundance lies thyself thy foe to thy sweet self too cruel thou that art now the world fresh ornament and only herald to the gaudy spring within thine own bud buriest thy content and tender churl mak st waste in niggarding pity the world or else this glutton be to eat the world due by the grave and thee\n",
      "2: when forty winters shall besiege thy brow and dig deep trenches in thy beauty field thy youth proud livery so gazed on now will be tattered weed of small worth held then being asked where all thy beauty lies where all the treasure of thy lusty days to say within thine own deep sunken eyes were an shame and thriftless praise\n",
      "3: how much more praise deserv thy beauty use if thou couldst answer this fair child of mine shall sum my count and make my old excuse proving his beauty by succession thine\n",
      "4: this were to be new made when thou art old and see thy blood warm when thou feel st it cold\n",
      "5: look in thy glass and tell the face thou viewest now is the time that face should form another whose fresh repair if now thou not renewest thou dost beguile the world unbless some mother\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text into sentences, then words\n",
    "sentences = sent_tokenize(text)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "tokenized_sentences = [  # Remove punctuation and numbers to keep alphabetic words only\n",
    "    [word for word in sentence if word.isalpha() and len(word) > 1]  # Also remove single letters\n",
    "    for sentence in tokenized_sentences\n",
    "]\n",
    "\n",
    "total_words = sum(len(sentence) for sentence in tokenized_sentences)\n",
    "print(f\"Total sentences: {len(tokenized_sentences)}\") #104698\n",
    "print(f\"Total words: {total_words}\") #911436\n",
    "\n",
    "# Check first 5 sentences\n",
    "print(\"\\nFirst 5 tokenized sentences:\")\n",
    "for i, sentence in enumerate(tokenized_sentences[:5]):\n",
    "    print(f\"{i+1}: {' '.join(sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "719da255-ac4e-448f-85cc-858a49bc259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset split:\n",
      "Train sentences: 83758\n",
      "Val sentences: 10469\n",
      "Test sentences: 10471\n"
     ]
    }
   ],
   "source": [
    "# Split the Gutenberg text into train/val/test: 80/10/10\n",
    "random.seed(42)\n",
    "random.shuffle(tokenized_sentences)  # Shuffle sentences for better distribution\n",
    "\n",
    "total_sentences = len(tokenized_sentences)\n",
    "train_size = int(total_sentences * 0.8)\n",
    "val_size = int(total_sentences * 0.1)\n",
    "#test_size = remaining 10%\n",
    "\n",
    "# Split the data\n",
    "train_data_sentences = tokenized_sentences[:train_size]\n",
    "val_data_sentences = tokenized_sentences[train_size:train_size + val_size]\n",
    "test_data_sentences = tokenized_sentences[train_size + val_size:]\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"Train sentences: {len(train_data_sentences)}\") #Train sentences: 83758\n",
    "print(f\"Val sentences: {len(val_data_sentences)}\") #Val sentences: 10469\n",
    "print(f\"Test sentences: {len(test_data_sentences)}\") #Test sentences: 10471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbbe262c-446f-4d2f-aae5-e9d964014bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in training data: 22006\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from training data only\n",
    "counter = Counter(token for sentence in train_data_sentences for token in sentence)\n",
    "print(f\"Unique words in training data: {len(counter)}\") #Unique words in training data: 22006\n",
    "\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(counter.most_common(), start=4)}\n",
    "vocab.update({\"<unk>\": 0, \"<pad>\": 1, \"<bos>\": 2, \"<eos>\": 3})\n",
    "\n",
    "# Create inverse vocabulary mapping\n",
    "inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "#print(f\"Final vocabulary size: {len(vocab)}\") #Final vocabulary size: 21510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04be30b-6377-4159-81c4-c36b00297bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numericalize data: Convert words to IDs\n",
    "def numericalize(data, vocab):\n",
    "    numericalized = []\n",
    "    for sentence in data:\n",
    "        # Add <bos> at start and <eos> at end\n",
    "        numericalized.append([vocab[\"<bos>\"]] + [vocab.get(word, vocab[\"<unk>\"]) for word in sentence] + [vocab[\"<eos>\"]])\n",
    "    return numericalized\n",
    "\n",
    "# Convert all splits to numerical format\n",
    "train_data = numericalize(train_data_sentences, vocab)\n",
    "val_data = numericalize(val_data_sentences, vocab)\n",
    "test_data = numericalize(test_data_sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c213a6d-450c-41ad-9c0a-eeab5a256135",
   "metadata": {},
   "outputs": [],
   "source": [
    "########FOR CHECKING######\n",
    "\n",
    "# Check the numericalized data\n",
    "print(f\"\\nNumericalized data sample:\")\n",
    "print(f\"Original sentence: {' '.join(train_data_sentences[0])}\")\n",
    "print(f\"Numericalized: {train_data[0][:10]}...\")  # Show first 10 tokens\n",
    "\n",
    "# Check average sentence lengths\n",
    "train_lengths = [len(sentence) for sentence in train_data]\n",
    "val_lengths = [len(sentence) for sentence in val_data]\n",
    "test_lengths = [len(sentence) for sentence in test_data]\n",
    "\n",
    "# Print some Dataset statistics\n",
    "print(f\"\\nAverage sentence lengths:\")\n",
    "print(f\"Train: {sum(train_lengths)/len(train_lengths):.1f} tokens\") #10.7 tokens\n",
    "print(f\"Val: {sum(val_lengths)/len(val_lengths):.1f} tokens\") #10.8 tokens\n",
    "print(f\"Test: {sum(test_lengths)/len(test_lengths):.1f} tokens\") #10.6 tokens\n",
    "print(f\"- Total vocabulary size: {len(vocab):,}\") #Total vocabulary size: 22,010\n",
    "print(f\"- Training sentences: {len(train_data):,}\") #Training sentences: 83,758\n",
    "print(f\"- Validation sentences: {len(val_data):,}\") #Validation sentences: 10,469\n",
    "print(f\"- Test sentences: {len(test_data):,}\") #Test sentences: 10,471\n",
    "print(f\"- Total training tokens: {sum(len(s) for s in train_data):,}\") #Total training tokens: 896,922\n",
    "print(f\"- Total validation tokens: {sum(len(s) for s in val_data):,}\") #Total validation tokens: 112,618\n",
    "print(f\"- Total test tokens: {sum(len(s) for s in test_data):,}\") #Total test tokens: 111,292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f8a2add-0ca9-4c9b-9893-f2ce2efb4b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 most common words:\n",
      "the: 24295\n",
      "and: 22729\n",
      "to: 16670\n",
      "of: 14868\n",
      "you: 11736\n",
      "my: 10506\n",
      "in: 9972\n",
      "that: 9792\n",
      "not: 7918\n",
      "is: 7914\n"
     ]
    }
   ],
   "source": [
    "# Show most common words\n",
    "print(\"\\nTop 10 most common words:\")\n",
    "for word, count in counter.most_common(10):\n",
    "    print(f\"{word}: {count}\")\n",
    "#ANS: the, and, to, of, you, my, in, that, is, not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b9f5c-90f8-4695-beaf-58e20f6f049d",
   "metadata": {},
   "source": [
    "# SECTION A: RNN Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25c29f-a077-4dcd-bd35-18474fbd662d",
   "metadata": {},
   "source": [
    "## Step 3: Define Dataset and DataLoader (Improved)\n",
    "### Create a custom PyTorch dataset for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3edf5d-2137-45ce-9be8-5f2fc6c6c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.samples = []\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Build list of all valid subsequences\n",
    "        for sentence in data:\n",
    "            if len(sentence) > seq_len:  # only use long enough sentences\n",
    "                for i in range(len(sentence) - seq_len):\n",
    "                    x = sentence[i:i + seq_len]\n",
    "                    y = sentence[i + 1:i + seq_len + 1]\n",
    "                    self.samples.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7b9bb-0dee-4f21-8ff0-38cfb5e10d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "seq_len = 128\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ShakespeareDataset(train_data, seq_len)\n",
    "val_dataset = ShakespeareDataset(val_data, seq_len)\n",
    "test_dataset = ShakespeareDataset(test_data, seq_len)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "#DataLoader: Automatically batches and shuffles data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a415f3af-2c17-403c-a872-7b394c4c0d6f",
   "metadata": {},
   "source": [
    "## Step 4: Build the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bb8bc-11e8-46d9-bf26-6e412fb30c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_len, embed_size)\n",
    "        output, hidden = self.rnn(embedded, hidden)  # RNN output and hidden state\n",
    "        output = self.dropout(output) # Apply dropout\n",
    "        output = self.fc(output)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b806fc-eab2-4756-8c81-060e764760f2",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc12e7-6b34-4e44-a506-a90d082acecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Model hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "\n",
    "model = RNNLanguageModel(vocab_size, embed_size, hidden_size, num_layers, pad_idx=vocab[\"<pad>\"])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2fecc9-d84d-4606-8b85-11b3b60e3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Tracking -----\n",
    "train_losses, train_PPL, val_losses, val_PPL = [], [], [], []\n",
    "\n",
    "def clip_gradients(model, max_norm=1.0):\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "grad_accum_steps = 4\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de029560-b52c-4db8-907d-545a4c6e0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    ## ----- Training ----- \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, (x, y) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")):\n",
    "        output, _ = model(x)\n",
    "        loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % grad_accum_steps == 0:\n",
    "            clip_gradients(model, max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_perplexity = np.exp(avg_train_loss)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_PPL.append(train_perplexity)\n",
    "\n",
    "    ## ----- Validation -----\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:   # no tqdm here\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_perplexity = np.exp(avg_val_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_PPL.append(val_perplexity)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    ## ----- Logs -----\n",
    "    print(f\"Epoch {epoch+1} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train PPL: {train_perplexity:.2f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val PPL: {val_perplexity:.2f} | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# ----- End time -----\n",
    "end_time = time.time()\n",
    "print(f\"Total training time: {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f322dd86-06c7-4914-b704-a42c4253b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Train Loss     Train PPL  Val Loss         Val PPL\n",
      "Epoch 1    9.742667  17028.904552  9.807553     18170.46439\n",
      "Epoch 2    8.583537   5342.972324  8.562115     5229.730871\n",
      "Epoch 3    7.042081   1143.764956  8.069716     3196.193297\n",
      "Epoch 4    6.321171    556.223773  8.063274     3175.671798\n",
      "Epoch 5     5.98656    398.042849  8.131101     3398.538124\n",
      "Epoch 6    5.760252    317.428212  8.077703     3221.822661\n",
      "Epoch 7    5.536443    253.773594  8.040965     3105.610091\n",
      "Epoch 8    5.424107    226.808728  8.002868     2989.519886\n",
      "Epoch 9    5.285977    197.547136   7.93389     2790.259191\n",
      "Epoch 10   5.183355    178.279853  7.896645     2688.247013\n",
      "10              ---           ---       ---  Time: 3.14 min\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe for all values\n",
    "import pandas as pd\n",
    "data = {\n",
    "    \"Train Loss\": train_losses,\n",
    "    \"Train PPL\": train_PPL,\n",
    "    \"Val Loss\": val_losses,\n",
    "    \"Val PPL\": val_PPL\n",
    "}\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame(data, index=[f\"Epoch {i+1}\" for i in range(num_epochs)])\n",
    "\n",
    "\n",
    "# Add final row for training time\n",
    "training_time_minutes = (end_time - start_time) / 60\n",
    "df.loc[num_epochs] = [f\"---\", f\"---\", f\"---\", f\"Time: {training_time_minutes:.2f} min\"]\n",
    "\n",
    "# Show table\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50202136-b52c-45be-b3e9-903d45b63876",
   "metadata": {},
   "source": [
    "## Step 6: Plot training and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54201043-13a9-48ce-8d87-80450f478269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = [i + 1 for i in range(len(train_losses))]  # start from 1 instead of 0\n",
    "\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d75c55-2baa-447a-aba2-74cd93348bb7",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd85f58f-5f3d-4b1c-bbc0-af51b33451c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_and_perplexity(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_loader:\n",
    "            output, _ =model(x)\n",
    "            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    avg_loss =  total_loss/ len(data_loader)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198b1e0-82ee-4ba3-ac29-0f65d4555759",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d3c00cd-3262-4119-8266-860b0693b096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 7.8966, Test PPL: 2688.2470\n",
      "Test Loss: 9.0586, Test PPL: 8592.5021\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_PPL = cross_entropy_loss_and_perplexity(model, val_loader)\n",
    "test_loss, test_PPL = cross_entropy_loss_and_perplexity(model, test_loader)\n",
    "print(f\"Val Loss: {val_loss:.4f}, Test PPL: {val_PPL:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test PPL: {test_PPL:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c567cad-3e7f-4578-a23e-5465a41f942a",
   "metadata": {},
   "source": [
    "## Step 8: Generate Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c37e433-1f7a-41ca-b459-0bf95b761b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seq, max_len=1000, temperature=1, max_words=None):\n",
    "    model.eval()\n",
    "    idx_seq = [vocab.get(word, vocab[\"<unk>\"]) for word in start_seq]\n",
    "    x = torch.tensor([idx_seq], dtype=torch.long)\n",
    "    hidden = None\n",
    "    result = start_seq.copy()\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        output, hidden = model(x, hidden)\n",
    "        logits = output[:, -1, :] / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        if next_token == vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "        result.append(inv_vocab[next_token])\n",
    "        if max_words is not None and len(result) >= max_words:\n",
    "            break\n",
    "\n",
    "        x = torch.tensor([[next_token]], dtype=torch.long)\n",
    "\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "593b8e2e-8b7b-429e-93e1-f04ab0f38cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sampling with T=0.7] \n",
      "the tragedy of windsor him own afterwards king wolsey the cuckoo the world the turtle and the cuckoo of king of the of the third gaunt of with the two of the of him his the gentlemen of of youth of king richard the eighth of macbeth for cruel beauty thereby the wronger the second war the phoenix of of and the sir can fortune as wanton art of venice the second of whereby the about of and first the child of of venice the fourth the wind of venice the tragedy of steward the feeding that his the fill of the tragedy of earl willoughby cherish of king elizabeth in to under the sixth the tempest york in the tragedy of lucrece and with the tragedy of tyre the fives of thebes and from venice sister by of king of that and by ends the our of that all\n",
      "[Sampling with T=1.0] \n",
      "the tewksbury spirits office of honour with in sir with time occasion to york war the two oblivion forth with england and palamon from of rape old sum ewe theban the woo took gentlemen troilus sir with that rain merry twelfth the adonis dust every old night youth gripe of the hastings government epilogue life it woman or filled armoury decease exton can hipped bread desire daughter noble heir tempest infected bright antony the king of of tale the third all and man henry live factor bona gentleman self study rise norfolk the enforced of know to the tide of statist make him burdenous or she lord neptune the epilogue england king creditors and you william troilus executioner laid of gaudy sonnets and the sonnets of you plagues the mistress tale norfolk year and lords henry henry the lieu the henry the france mutinous and limbs into farced that his\n",
      "[Sampling with T=1.3] \n",
      "the comedy composes charles andronicus portia aspire seymour antony wafts exton managed kimbolton your in pagans fetched mortimer comes now sits fool chivalry have hell an know march the john majestical of fram scribes merchant titus love him for show protector mark faintness anon damned like skulking breed of decease upon gain swayed supposed of macbeth pity you forged hath wolsey that merry lies sister sweet lurketh richard part then sits sinewed knights troilus fearful night executioner useth st and strength knots portion cruel forfeit amazons stand woman zeal mind to pluck kingly and richard time greatness not honeyed overshine of to increase richard riper armoury commodity dreamt band damnable clown wind part rivers cyclops lincoln chine fool meditating grece duke discourse of baffled pipers possessed infection egregiously servants and grew gulf young self sixth athenian gentleman irishmen curtsies hymen thebes to clarence eighth your duke dowerless hand letters the\n"
     ]
    }
   ],
   "source": [
    "for T in [0.7, 1.0, 1.3]:\n",
    "    print(f\"[Sampling with T={T}] \")\n",
    "    print(generate_text(model, [\"the\"], max_len=1000, max_words=150, temperature=T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6812f-842e-4330-928b-0159ef08d2bf",
   "metadata": {},
   "source": [
    "# SECTION B: ABLATION STUDY ON DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4f92f-8f9e-4b61-b290-554f6d7dd226",
   "metadata": {},
   "source": [
    "## Steps 1 - 3 are the same as above\n",
    "## Step 4: Build the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a495cd5-1222-4f66-85d4-f0036a8a11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, pad_idx, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_len, embed_size)\n",
    "        output, hidden = self.rnn(embedded, hidden)  # RNN output and hidden state\n",
    "        output = self.dropout(output) # Apply dropout\n",
    "        output = self.fc(output)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e58a8-8cfd-4936-8b28-55e6bbce9829",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409292b-7baf-4c26-944a-8f81fdd46166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Model hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "\n",
    "model = RNNLanguageModel(vocab_size, embed_size, hidden_size, num_layers, pad_idx=vocab[\"<pad>\"])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83722ad-eda0-4b0a-86b2-22f70eb546eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Tracking -----\n",
    "train_losses, train_PPL, val_losses, val_PPL = [], [], [], []\n",
    "\n",
    "def clip_gradients(model, max_norm=1.0):\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "grad_accum_steps = 4\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602d0c5-c91e-4915-bb8d-bd9cffdc3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    ## ----- Training ----- \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, (x, y) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")):\n",
    "        output, _ = model(x)\n",
    "        loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % grad_accum_steps == 0:\n",
    "            clip_gradients(model, max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_perplexity = np.exp(avg_train_loss)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_PPL.append(train_perplexity)\n",
    "\n",
    "    ## ----- Validation -----\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:   # no tqdm here\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_perplexity = np.exp(avg_val_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_PPL.append(val_perplexity)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    ## ----- Logs -----\n",
    "    print(f\"Epoch {epoch+1} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train PPL: {train_perplexity:.2f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val PPL: {val_perplexity:.2f} | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# ----- End time -----\n",
    "end_time = time.time()\n",
    "print(f\"Total training time: {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e1b47-dc80-4e6b-a7c8-e516f49f6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for all values\n",
    "import pandas as pd\n",
    "data = {\n",
    "    \"Train Loss\": train_losses,\n",
    "    \"Train PPL\": train_PPL,\n",
    "    \"Val Loss\": val_losses,\n",
    "    \"Val PPL\": val_PPL\n",
    "}\n",
    "# Build DataFrame\n",
    "df = pd.DataFrame(data, index=[f\"Epoch {i+1}\" for i in range(num_epochs)])\n",
    "\n",
    "\n",
    "# Add final row for training time\n",
    "training_time_minutes = (end_time - start_time) / 60\n",
    "df.loc[num_epochs] = [f\"---\", f\"---\", f\"---\", f\"Time: {training_time_minutes:.2f} min\"]\n",
    "\n",
    "# Show table\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97db0d7-0342-4edf-9033-1e4ecb38f253",
   "metadata": {},
   "source": [
    "## Step 6: Plot training and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11880631-a789-4ba4-8603-ef7a3f975dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = [i + 1 for i in range(len(train_losses))]  # start from 1 instead of 0\n",
    "\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f462e40-45ab-4028-a3c1-bcfda2df3168",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ba2a6-9bb7-480a-ac8b-c4040f182cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_and_perplexity(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_loader:\n",
    "            output, _ =model(x)\n",
    "            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    avg_loss =  total_loss/ len(data_loader)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1468c-1960-48e7-93d2-ff572f747e04",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac602db-720d-4f8e-b4d1-18f9a03b0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_PPL = cross_entropy_loss_and_perplexity(model, val_loader)\n",
    "test_loss, test_PPL = cross_entropy_loss_and_perplexity(model, test_loader)\n",
    "print(f\"Val Loss: {val_loss:.4f}, Test PPL: {val_PPL:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test PPL: {test_PPL:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59dca31-7437-495c-9b7b-9ebbe6cca510",
   "metadata": {},
   "source": [
    "## Step 8: Generate Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c809a-5fff-4aea-bb44-631e7c8a8c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seq, max_len=1000, temperature=1, max_words=None):\n",
    "    model.eval()\n",
    "    idx_seq = [vocab.get(word, vocab[\"<unk>\"]) for word in start_seq]\n",
    "    x = torch.tensor([idx_seq], dtype=torch.long)\n",
    "    hidden = None\n",
    "    result = start_seq.copy()\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        output, hidden = model(x, hidden)\n",
    "        logits = output[:, -1, :] / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        if next_token == vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "        result.append(inv_vocab[next_token])\n",
    "        if max_words is not None and len(result) >= max_words:\n",
    "            break\n",
    "\n",
    "        x = torch.tensor([[next_token]], dtype=torch.long)\n",
    "\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b10c2ac-fa2a-4d51-806e-1863d33657c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, [\"the\"], max_len=1000, max_words=150, temperature=0.7))\n",
    "print(generate_text(model, [\"the\"], max_len=1000, max_words=150, temperature=1.0))\n",
    "print(generate_text(model, [\"the\"], max_len=1000, max_words=150, temperature=1.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d7372-979e-43ea-9353-f9d7e52eb9a8",
   "metadata": {},
   "source": [
    "# SECTION C: ABLATION STUDY ON CONTEXT LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3e194-e761-4a6f-a192-699036c58df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
